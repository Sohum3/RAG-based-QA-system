{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0ysHKM1omeW"
      },
      "source": [
        "# RAG (Retrieval-Augmented Generation) based QA system"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# Load sample dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:10000]\")  # 1000 articles\n",
        "docs = dataset[\"text\"][:5000]  # Taking 500 docs for efficiency\n",
        "\n",
        "# Embed using Sentence Transformer\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "doc_embeddings = embedder.encode(docs, convert_to_numpy=True)\n",
        "\n",
        "# Tokenize documents for BM25\n",
        "tokenized_docs = [doc.split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "def retrieve_documents(query, k=3):\n",
        "    tokenized_query = query.split()\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    top_indices = np.argsort(scores)[::-1][:k]\n",
        "    return [docs[i] for i in top_indices]\n",
        "\n",
        "# Load HuggingFace Model for Generation\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "def generate_answer(question):\n",
        "    retrieved_docs = retrieve_documents(question)\n",
        "    if not retrieved_docs:\n",
        "        return \"No relevant documents found.\"\n",
        "\n",
        "    context = \" \".join(retrieved_docs)  # Combine retrieved documents\n",
        "    input_text = f\"Context: {context} Question: {question}\"\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_length=100, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example Usage\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "IF2TzIXBN7pK",
        "outputId": "9099ec7d-1dfb-4267-8a44-4c9e890ee1c2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7e9ceaa5940e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Embed using Sentence Transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence-transformers/all-mpnet-base-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdoc_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Tokenize documents for BM25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m                     \u001b[0;31m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m                         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mall_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    question = input(\"Enter your question: \")\n",
        "    answer = generate_answer(question)\n",
        "    print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56hp8T8nN7jf",
        "outputId": "fc8236d6-a817-45e0-8e8d-65fbca131b17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: What is the capital of France?\n",
            "Answer: Nova Scotia is a small province on the east coast of Canada. The capital and largest city is Halifax. There are over 900,000 people who live in Nova Scotia, called Nova Scotians. The name 'Nova' is Latin for 'New Scotland' and means 'love of wisdom'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}